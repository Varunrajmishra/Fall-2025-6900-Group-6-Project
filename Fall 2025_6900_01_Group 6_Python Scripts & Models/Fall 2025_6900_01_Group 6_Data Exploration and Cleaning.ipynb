{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3c2d56-1ebe-4fdf-b6f7-36d63e7124dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kanha\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kanha\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kanha\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kanha\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kanha\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: azure-ai-ml in c:\\users\\kanha\\anaconda3\\lib\\site-packages (1.29.0)\n",
      "Requirement already satisfied: azureml-sdk in c:\\users\\kanha\\anaconda3\\lib\\site-packages (1.0.23)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (6.0.2)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (1.36.0)\n",
      "Requirement already satisfied: azure-mgmt-core>=1.3.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (1.6.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (3.26.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (4.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (4.67.1)\n",
      "Requirement already satisfied: strictyaml<2.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (1.7.3)\n",
      "Requirement already satisfied: colorama<1.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (0.4.6)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (2.10.1)\n",
      "Requirement already satisfied: azure-storage-blob>=12.10.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (12.27.0)\n",
      "Requirement already satisfied: azure-storage-file-share in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (12.23.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (12.22.0)\n",
      "Requirement already satisfied: pydash<9.0.0,>=6.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (8.0.5)\n",
      "Requirement already satisfied: isodate<1.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (0.7.2)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: typing-extensions<5.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (4.12.2)\n",
      "Requirement already satisfied: azure-monitor-opentelemetry in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-ai-ml) (1.8.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.22.3)\n",
      "Requirement already satisfied: azureml-core==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: azureml-train==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: azureml-pipeline==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: backports.tempfile in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (1.0)\n",
      "Requirement already satisfied: pathspec in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.10.3)\n",
      "Requirement already satisfied: requests>=2.19.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (2.32.3)\n",
      "Requirement already satisfied: azure-mgmt-resource>=1.2.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (24.0.0)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (14.0.0)\n",
      "Requirement already satisfied: azure-mgmt-storage>=1.5.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (23.1.0)\n",
      "Requirement already satisfied: azure-mgmt-keyvault>=0.40.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (12.1.1)\n",
      "Requirement already satisfied: azure-mgmt-authorization>=0.40.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (4.0.0)\n",
      "Requirement already satisfied: azure-graphrbac>=0.40.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.61.2)\n",
      "Requirement already satisfied: msrest>=0.5.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.7.1)\n",
      "Requirement already satisfied: msrestazure>=0.4.33 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.6.4.post1)\n",
      "Requirement already satisfied: urllib3>=1.23 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (2.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (1.17.0)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (44.0.1)\n",
      "Requirement already satisfied: ndg-httpsclient in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.5.1)\n",
      "Requirement already satisfied: SecretStorage in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (3.4.0)\n",
      "Requirement already satisfied: ruamel.yaml<=0.15.89,>=0.15.35 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (0.15.89)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (4.1.1)\n",
      "Requirement already satisfied: contextlib2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (21.6.0)\n",
      "Requirement already satisfied: docker in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (7.1.0)\n",
      "Requirement already satisfied: adal>=1.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (1.2.7)\n",
      "Requirement already satisfied: paramiko>=2.0.8 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (4.0.0)\n",
      "Requirement already satisfied: pyopenssl in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (25.0.0)\n",
      "Requirement already satisfied: jmespath in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-core==1.0.23.*->azureml-sdk) (1.0.1)\n",
      "Requirement already satisfied: azureml-pipeline-core==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-pipeline==1.0.23.*->azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: azureml-pipeline-steps==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-pipeline==1.0.23.*->azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: certifi in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-pipeline-steps==1.0.23.*->azureml-pipeline==1.0.23.*->azureml-sdk) (2025.8.3)\n",
      "Requirement already satisfied: azureml-train-core==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-pipeline-steps==1.0.23.*->azureml-pipeline==1.0.23.*->azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: azureml-train-restclients-hyperdrive==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-train-core==1.0.23.*->azureml-pipeline-steps==1.0.23.*->azureml-pipeline==1.0.23.*->azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: azureml-telemetry==1.0.23.* in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-train-core==1.0.23.*->azureml-pipeline-steps==1.0.23.*->azureml-pipeline==1.0.23.*->azureml-sdk) (1.0.23)\n",
      "Requirement already satisfied: applicationinsights in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azureml-telemetry==1.0.23.*->azureml-train-core==1.0.23.*->azureml-pipeline-steps==1.0.23.*->azureml-pipeline==1.0.23.*->azureml-sdk) (0.11.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests>=2.19.1->azureml-core==1.0.23.*->azureml-sdk) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests>=2.19.1->azureml-core==1.0.23.*->azureml-sdk) (3.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from msrest>=0.5.1->azureml-core==1.0.23.*->azureml-sdk) (2.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.23.*->azureml-sdk) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.23.*->azureml-sdk) (2.21)\n",
      "Requirement already satisfied: bcrypt>=3.2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from paramiko>=2.0.8->azureml-core==1.0.23.*->azureml-sdk) (4.3.0)\n",
      "Requirement already satisfied: invoke>=2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from paramiko>=2.0.8->azureml-core==1.0.23.*->azureml-sdk) (2.2.1)\n",
      "Requirement already satisfied: pynacl>=1.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from paramiko>=2.0.8->azureml-core==1.0.23.*->azureml-sdk) (1.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-core==1.0.23.*->azureml-sdk) (3.3.1)\n",
      "Requirement already satisfied: azure-core-tracing-opentelemetry~=1.0.0b11 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b12)\n",
      "Requirement already satisfied: azure-monitor-opentelemetry-exporter~=1.0.0b41 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.0.0b44)\n",
      "Requirement already satisfied: opentelemetry-sdk~=1.36 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-django~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-flask~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-psycopg2~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-requests~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-urllib~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-urllib3~=0.57b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-resource-detector-azure~=0.1.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry->azure-ai-ml) (0.1.5)\n",
      "Requirement already satisfied: opentelemetry-api>=1.12.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (1.38.0)\n",
      "Requirement already satisfied: azure-identity~=1.17 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (1.25.1)\n",
      "Requirement already satisfied: fixedint==0.1.6 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (0.1.6)\n",
      "Requirement already satisfied: psutil<8,>=5.9 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (5.9.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-identity~=1.17->azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from azure-identity~=1.17->azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (3.21.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-wsgi==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (1.17.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.59b0->opentelemetry-instrumentation-fastapi~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (3.10.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-dbapi==0.59b0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-psycopg2~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (0.59b0)\n",
      "Requirement already satisfied: backports.weakref in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from backports.tempfile->azureml-core==1.0.23.*->azureml-sdk) (1.0.post1)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from docker->azureml-core==1.0.23.*->azureml-sdk) (308)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from ndg-httpsclient->azureml-core==1.0.23.*->azureml-sdk) (0.4.8)\n",
      "Requirement already satisfied: jeepney>=0.6 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from SecretStorage->azureml-core==1.0.23.*->azureml-sdk) (0.9.0)\n",
      "‚úÖ All packages installed successfully!\n",
      "üìä Ready to start data analysis!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn azure-ai-ml azureml-sdk\n",
    "\n",
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"üìä Ready to start data analysis!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248a8c91-b247-44fc-9f4c-b5e5c3f1b3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading datasets...\n",
      "‚úÖ Traffic data loaded: 5383378 rows, 30 columns\n",
      "‚úÖ Passenger data loaded: 2786 rows, 4 columns\n",
      "‚úÖ Bus data loaded: 2784 rows, 4 columns\n",
      "‚úÖ Mobility speeds data loaded: 192 rows, 7 columns\n",
      "\n",
      "üéâ All datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load your datasets with ACTUAL file names\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "\n",
    "# 1. Load Traffic Data (the huge 6M+ row dataset - you're using the sample)\n",
    "df_traffic = pd.read_csv('../data/All Recorded Traffic.txt', sep='\\t')\n",
    "print(f\"‚úÖ Traffic data loaded: {df_traffic.shape[0]} rows, {df_traffic.shape[1]} columns\")\n",
    "\n",
    "# 2. Load PABT Passenger Data  \n",
    "df_passenger = pd.read_csv('../data/All Recorded PABT Passenger.txt', sep='\\t')\n",
    "print(f\"‚úÖ Passenger data loaded: {df_passenger.shape[0]} rows, {df_passenger.shape[1]} columns\")\n",
    "\n",
    "# 3. Load PABT Bus Data\n",
    "df_bus = pd.read_csv('../data/All Recorded PABT Bus.txt', sep='\\t') \n",
    "print(f\"‚úÖ Bus data loaded: {df_bus.shape[0]} rows, {df_bus.shape[1]} columns\")\n",
    "\n",
    "# 4. Load Facility Mobility Speeds Data\n",
    "df_speeds = pd.read_csv('../data/Facility Mobility Speeds.txt', sep='\\t')\n",
    "print(f\"‚úÖ Mobility speeds data loaded: {df_speeds.shape[0]} rows, {df_speeds.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nüéâ All datasets loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26459623-8aa4-4ba9-843c-fd8f1732e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Parsing dates...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÖ Parsing dates...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Traffic data - convert Date column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m df_traffic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_traffic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# PABT Bus data - convert Start_Date and End_Date\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_bus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_bus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "# Parse dates properly based on the data dictionary\n",
    "print(\"üìÖ Parsing dates...\")\n",
    "\n",
    "# Traffic data - convert Date column\n",
    "df_traffic['Date'] = pd.to_datetime(df_traffic['Date'])\n",
    "\n",
    "# PABT Bus data - convert Start_Date and End_Date\n",
    "df_bus['Start_Date'] = pd.to_datetime(df_bus['Start_Date'])\n",
    "df_bus['End_Date'] = pd.to_datetime(df_bus['End_Date'])\n",
    "\n",
    "# PABT Passenger data - convert Start_Date and End_Date  \n",
    "df_passenger['Start_Date'] = pd.to_datetime(df_passenger['Start_Date'])\n",
    "df_passenger['End_Date'] = pd.to_datetime(df_passenger['End_Date'])\n",
    "\n",
    "# Facility Mobility Speeds - convert Month_Year if it exists\n",
    "if 'Month_Year' in df_speeds.columns:\n",
    "    df_speeds['Month_Year'] = pd.to_datetime(df_speeds['Month_Year'])\n",
    "\n",
    "print(\"‚úÖ All dates parsed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a904436f-3131-449c-a2ff-95821fe8abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING ACTUAL COLUMN NAMES\n",
      "==================================================\n",
      "üìä TRAFFIC DATA COLUMNS:\n",
      "Number of columns: 30\n",
      "Column names: ['DAY', 'DATE', 'FAC', 'LANE', 'TIME', 'TOTAL', 'CLASS 1', 'CLASS 2', 'CLASS 3', 'CLASS 4', 'CLASS 5', 'CLASS 6', 'CLASS 7', 'CLASS 8', 'CLASS 9', 'CLASS 11', 'CASH', 'EZPASS', 'VIOLATION', 'LANEMODE', 'Month', 'FAC_B', 'Autos', 'Small_T', 'Large_T', 'Buses', 'Yr', 'FAC_G', 'FAC_G2', 'Day_Name']\n",
      "\n",
      "First few rows to understand structure:\n",
      "   DAY                     DATE  FAC  LANE  TIME  TOTAL  CLASS 1  CLASS 2  \\\n",
      "0    4  2022-09-15 00:00:00.000    2     6   100      1      1.0      NaN   \n",
      "1    4  2022-09-15 00:00:00.000    2     6   300      1      NaN      NaN   \n",
      "\n",
      "   CLASS 3  CLASS 4  ...  Month    FAC_B  Autos  Small_T  Large_T  Buses  \\\n",
      "0      NaN      NaN  ...      9  Lincoln      1        0        0      0   \n",
      "1      NaN      NaN  ...      9  Lincoln      1        0        0      0   \n",
      "\n",
      "     Yr    FAC_G   FAC_G2  Day_Name  \n",
      "0  2022  Tunnels  Lincoln  Thursday  \n",
      "1  2022  Tunnels  Lincoln  Thursday  \n",
      "\n",
      "[2 rows x 30 columns]\n",
      "\n",
      "==================================================\n",
      "üöå BUS DATA COLUMNS:\n",
      "Column names: ['Start_Date', 'End_Date', 'Carrier', 'Volume']\n",
      "\n",
      "==================================================\n",
      "üë• PASSENGER DATA COLUMNS:\n",
      "Column names: ['Start_Date', 'End_Date', 'Carrier', 'Volume']\n",
      "\n",
      "==================================================\n",
      "üöó MOBILITY SPEEDS DATA COLUMNS:\n",
      "Column names: ['Facility', 'Freeflow', 'Month_Year', 'Avg_Speed', 'Direction', 'Delta', 'Facility_Order']\n"
     ]
    }
   ],
   "source": [
    "# Check what columns actually exist in each dataset\n",
    "print(\"üîç CHECKING ACTUAL COLUMN NAMES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìä TRAFFIC DATA COLUMNS:\")\n",
    "print(f\"Number of columns: {len(df_traffic.columns)}\")\n",
    "print(\"Column names:\", list(df_traffic.columns))\n",
    "print(\"\\nFirst few rows to understand structure:\")\n",
    "print(df_traffic.head(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöå BUS DATA COLUMNS:\")\n",
    "print(\"Column names:\", list(df_bus.columns))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üë• PASSENGER DATA COLUMNS:\")\n",
    "print(\"Column names:\", list(df_passenger.columns))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöó MOBILITY SPEEDS DATA COLUMNS:\")\n",
    "print(\"Column names:\", list(df_speeds.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9207cc-e780-48d1-9682-d2891a4ebb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Parsing dates with correct column names...\n",
      "‚úÖ Traffic dates parsed\n",
      "‚úÖ Bus dates parsed\n",
      "‚úÖ Passenger dates parsed\n",
      "‚úÖ Mobility speeds dates parsed\n",
      "üéâ All dates parsed successfully!\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED DATE PARSING with actual column names\n",
    "print(\"üìÖ Parsing dates with correct column names...\")\n",
    "\n",
    "# Traffic data - use 'DATE' (uppercase)\n",
    "df_traffic['DATE'] = pd.to_datetime(df_traffic['DATE'])\n",
    "print(\"‚úÖ Traffic dates parsed\")\n",
    "\n",
    "# PABT Bus data - convert Start_Date and End_Date\n",
    "df_bus['Start_Date'] = pd.to_datetime(df_bus['Start_Date'])\n",
    "df_bus['End_Date'] = pd.to_datetime(df_bus['End_Date'])\n",
    "print(\"‚úÖ Bus dates parsed\")\n",
    "\n",
    "# PABT Passenger data - convert Start_Date and End_Date  \n",
    "df_passenger['Start_Date'] = pd.to_datetime(df_passenger['Start_Date'])\n",
    "df_passenger['End_Date'] = pd.to_datetime(df_passenger['End_Date'])\n",
    "print(\"‚úÖ Passenger dates parsed\")\n",
    "\n",
    "# Facility Mobility Speeds - convert Month_Year\n",
    "df_speeds['Month_Year'] = pd.to_datetime(df_speeds['Month_Year'])\n",
    "print(\"‚úÖ Mobility speeds dates parsed\")\n",
    "\n",
    "print(\"üéâ All dates parsed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97022d3c-5406-419d-ad26-deac77fef597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç EXAMINING TRAFFIC DATA - YOUR 6M+ ROW DATASET SAMPLE\n",
      "============================================================\n",
      "üìÖ Date range: 2013-01-01 00:00:00 to 2025-05-31 00:00:00\n",
      "üè¢ Facilities: ['Lincoln' 'Holland' 'Goethals' 'Bayonne' 'Outerbridge' 'GWB Upper'\n",
      " 'GWB Lower' 'GWB PIP']\n",
      "üö® Total violations in sample: 81105494\n",
      "üöó Total traffic volume in sample: 1454450250\n",
      "\n",
      "üéØ Traffic by Facility:\n",
      "                 TOTAL  VIOLATION      CASH     EZPASS\n",
      "FAC_B                                                 \n",
      "Bayonne       38821261    1888666   4297830   32634837\n",
      "GWB Lower    258169916   19417263  31556372  207196319\n",
      "GWB PIP       63754929    4192434   4106239   55456274\n",
      "GWB Upper    296961198   18516416  51165911  227281111\n",
      "Goethals     201253919    8168861  27676826  165408487\n",
      "Holland      185816893    8918942  32227218  144670811\n",
      "Lincoln      228982859   13094068  26497006  189392787\n",
      "Outerbridge  180689275    6908844  18229240  155551592\n",
      "\n",
      "‚è∞ Peak Traffic Times (sample):\n",
      "TIME\n",
      "1700    85524435\n",
      "1800    84489286\n",
      "1600    83010550\n",
      "600     81446373\n",
      "1500    80484332\n",
      "1400    78118935\n",
      "700     77782604\n",
      "900     77124423\n",
      "1000    76054034\n",
      "1300    75751048\n",
      "Name: TOTAL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç EXAMINING TRAFFIC DATA - YOUR 6M+ ROW DATASET SAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check data range and key insights\n",
    "print(f\"üìÖ Date range: {df_traffic['DATE'].min()} to {df_traffic['DATE'].max()}\")\n",
    "print(f\"üè¢ Facilities: {df_traffic['FAC_B'].unique()}\")\n",
    "print(f\"üö® Total violations in sample: {df_traffic['VIOLATION'].sum()}\")\n",
    "print(f\"üöó Total traffic volume in sample: {df_traffic['TOTAL'].sum()}\")\n",
    "\n",
    "# Key facilities analysis\n",
    "print(f\"\\nüéØ Traffic by Facility:\")\n",
    "facility_summary = df_traffic.groupby('FAC_B').agg({\n",
    "    'TOTAL': 'sum',\n",
    "    'VIOLATION': 'sum',\n",
    "    'CASH': 'sum', \n",
    "    'EZPASS': 'sum'\n",
    "}).round()\n",
    "print(facility_summary)\n",
    "\n",
    "# Peak times analysis (from your sample)\n",
    "print(f\"\\n‚è∞ Peak Traffic Times (sample):\")\n",
    "time_summary = df_traffic.groupby('TIME')['TOTAL'].sum().sort_values(ascending=False).head(10)\n",
    "print(time_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7a0dbe-7240-4987-a8ab-9110ac923c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç EXAMINING PABT PASSENGER DATA\n",
      "============================================================\n",
      "üìÖ Date range: 2020-12-07 00:00:00 to 2025-05-26 00:00:00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÖ Date range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_passenger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_passenger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöå Carriers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(df_passenger[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCarrier\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Top carriers by passenger volume[^7,^8]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müë• Top Carriers by Passenger Volume:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)  \n",
    "print(\"üîç EXAMINING PABT PASSENGER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÖ Date range: {df_passenger['Start_Date'].min()} to {df_passenger['Start_Date'].max()}\")\n",
    "print(f\"üöå Carriers: {sorted(df_passenger['Carrier'].unique())}\")\n",
    "\n",
    "# Top carriers by passenger volume[^7,^8]\n",
    "print(f\"\\nüë• Top Carriers by Passenger Volume:\")\n",
    "carrier_passengers = df_passenger.groupby('Carrier')['Volume'].sum().sort_values(ascending=False)\n",
    "print(carrier_passengers)\n",
    "\n",
    "# NJ Transit dominance (as expected from data)\n",
    "nj_transit_total = df_passenger[df_passenger['Carrier'] == 'NJTransit']['Volume'].sum()\n",
    "total_passengers = df_passenger['Volume'].sum()\n",
    "print(f\"\\nüéØ NJTransit Market Share: {nj_transit_total/total_passengers*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2875e6e-d336-4793-a252-3d53f8b4a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç EXAMINING PABT PASSENGER DATA\n",
      "============================================================\n",
      "üìÖ Date range: 2020-12-07 00:00:00 to 2025-05-26 00:00:00\n",
      "üöå Carriers: ['Academy', 'Academy ', 'C & J Bus Lines', 'C&J Bus Lines', 'Coach USA', 'DeCamp', 'Greyhound', 'HCEE - Community', 'Lakeland', 'Martz', 'NJ Transit', 'NJTransit', 'Peter Pan_Bonanza', 'Trailways', 'TransBridge']\n",
      "\n",
      "üë• Top Carriers by Passenger Volume:\n",
      "Carrier\n",
      "NJTransit            12985016.0\n",
      "NJ Transit            1452175.0\n",
      "Coach USA              972252.0\n",
      "HCEE - Community       330976.0\n",
      "Greyhound              320417.4\n",
      "Academy                245524.2\n",
      "Lakeland               240602.0\n",
      "Peter Pan_Bonanza      201468.0\n",
      "Martz                  160137.0\n",
      "TransBridge            129736.0\n",
      "Trailways              112371.0\n",
      "DeCamp                  42084.0\n",
      "Academy                  5643.0\n",
      "C&J Bus Lines              28.0\n",
      "C & J Bus Lines             0.0\n",
      "Name: Volume, dtype: float64\n",
      "\n",
      "üéØ NJTransit Market Share: 83.9%\n",
      "üìä Total Passengers (all carriers): 17,198,429.6\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)  \n",
    "print(\"üîç EXAMINING PABT PASSENGER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÖ Date range: {df_passenger['Start_Date'].min()} to {df_passenger['Start_Date'].max()}\")\n",
    "\n",
    "# Fix the carrier sorting issue by handling NaN values\n",
    "carriers_clean = df_passenger['Carrier'].dropna().unique()\n",
    "print(f\"üöå Carriers: {sorted(carriers_clean)}\")\n",
    "\n",
    "# Top carriers by passenger volume - handle NaN values\n",
    "print(f\"\\nüë• Top Carriers by Passenger Volume:\")\n",
    "carrier_passengers = df_passenger.groupby('Carrier', dropna=True)['Volume'].sum().sort_values(ascending=False)\n",
    "print(carrier_passengers)\n",
    "\n",
    "# NJ Transit dominance analysis - handle variations in naming\n",
    "nj_transit_variations = ['NJTransit', 'NJ Transit']\n",
    "nj_transit_total = 0\n",
    "for variation in nj_transit_variations:\n",
    "    if variation in df_passenger['Carrier'].values:\n",
    "        nj_transit_total += df_passenger[df_passenger['Carrier'] == variation]['Volume'].sum()\n",
    "\n",
    "total_passengers = df_passenger['Volume'].sum()\n",
    "print(f\"\\nüéØ NJTransit Market Share: {nj_transit_total/total_passengers*100:.1f}%\")\n",
    "print(f\"üìä Total Passengers (all carriers): {total_passengers:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67103134-e0d3-4c3a-88d6-a52e9ced5b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç EXAMINING PABT BUS DATA\n",
      "============================================================\n",
      "üìÖ Date range: 2020-12-07 00:00:00 to 2025-05-26 00:00:00\n",
      "\n",
      "üöå Top Carriers by Bus Count:\n",
      "Carrier\n",
      "NJ Transit           515189.0\n",
      "Coach USA             32892.0\n",
      "HCEE - Community      18901.0\n",
      "Greyhound             12029.0\n",
      "Academy                8419.0\n",
      "Lakeland               7180.0\n",
      "Peter Pan_Bonanza      6293.4\n",
      "Trailways              4712.0\n",
      "TransBridge            4413.0\n",
      "Martz                  4011.0\n",
      "DeCamp                 2954.0\n",
      "Academy                 179.0\n",
      "C & J Bus Lines          61.0\n",
      "Name: Volume, dtype: float64\n",
      "\n",
      "üìä Recent vs. Early Bus Volumes:\n",
      "2024+ Average: 238.1 buses/week\n",
      "Pre-2022 Average: 200.6 buses/week\n",
      "Growth: 18.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç EXAMINING PABT BUS DATA\") \n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÖ Date range: {df_bus['Start_Date'].min()} to {df_bus['Start_Date'].max()}\")\n",
    "\n",
    "# Bus volume analysis[^7]\n",
    "print(f\"\\nüöå Top Carriers by Bus Count:\")\n",
    "carrier_buses = df_bus.groupby('Carrier')['Volume'].sum().sort_values(ascending=False)\n",
    "print(carrier_buses)\n",
    "\n",
    "# Weekly trends\n",
    "print(f\"\\nüìä Recent vs. Early Bus Volumes:\")\n",
    "recent_buses = df_bus[df_bus['Start_Date'] >= '2024-01-01']['Volume'].mean()\n",
    "early_buses = df_bus[df_bus['Start_Date'] < '2022-01-01']['Volume'].mean()\n",
    "print(f\"2024+ Average: {recent_buses:.1f} buses/week\")\n",
    "print(f\"Pre-2022 Average: {early_buses:.1f} buses/week\")\n",
    "print(f\"Growth: {((recent_buses/early_buses)-1)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f23b79a-dd83-4732-a006-deb2ed4d37fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç EXAMINING FACILITY MOBILITY SPEEDS - CONGESTION DATA\n",
      "============================================================\n",
      "üìÖ Date range: 2024-01-01 00:00:00 to 2025-04-01 00:00:00\n",
      "üè¢ Facilities: ['BB' 'GB' 'GWB' 'HT' 'LT' 'OBX']\n",
      "\n",
      "üö® Congestion Analysis (Delta = Avg_Speed - Freeflow):\n",
      "          mean   min   max\n",
      "Facility                  \n",
      "BB         2.7   0.7   4.0\n",
      "GB        -0.1  -6.9   4.1\n",
      "GWB      -19.0 -27.3 -11.4\n",
      "HT        -8.5 -12.3  -1.3\n",
      "LT       -15.3 -19.7  -9.5\n",
      "OBX        1.1  -1.2   2.6\n",
      "\n",
      "üî¥ Most Congested Facilities (Worst Average Delta):\n",
      "Facility\n",
      "GWB   -19.009375\n",
      "LT    -15.340625\n",
      "HT     -8.531250\n",
      "GB     -0.100000\n",
      "OBX     1.125000\n",
      "Name: Delta, dtype: float64\n",
      "\n",
      "üìà 2025 Congestion by Facility:\n",
      "Facility\n",
      "GWB   -17.4\n",
      "LT    -12.6\n",
      "HT     -5.6\n",
      "GB      1.4\n",
      "OBX     1.5\n",
      "BB      2.9\n",
      "Name: Delta, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üîç EXAMINING FACILITY MOBILITY SPEEDS - CONGESTION DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÖ Date range: {df_speeds['Month_Year'].min()} to {df_speeds['Month_Year'].max()}\")\n",
    "print(f\"üè¢ Facilities: {df_speeds['Facility'].unique()}\")\n",
    "\n",
    "# Congestion analysis using Delta column[^45]\n",
    "print(f\"\\nüö® Congestion Analysis (Delta = Avg_Speed - Freeflow):\")\n",
    "congestion_summary = df_speeds.groupby('Facility')['Delta'].agg(['mean', 'min', 'max']).round(1)\n",
    "print(congestion_summary)\n",
    "\n",
    "# Most congested facilities\n",
    "print(f\"\\nüî¥ Most Congested Facilities (Worst Average Delta):\")\n",
    "worst_congestion = df_speeds.groupby('Facility')['Delta'].mean().sort_values().head()\n",
    "print(worst_congestion)\n",
    "\n",
    "# Recent congestion trends (2025 data)[^45]\n",
    "recent_congestion = df_speeds[df_speeds['Month_Year'] >= '2025-01-01']\n",
    "if len(recent_congestion) > 0:\n",
    "    print(f\"\\nüìà 2025 Congestion by Facility:\")\n",
    "    congestion_2025 = recent_congestion.groupby('Facility')['Delta'].mean().sort_values()\n",
    "    print(congestion_2025.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71748511-38c1-4b54-a9bf-cf8836a16ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® VIOLATION RATE ANALYSIS FOR AZURE ML\n",
      "==================================================\n",
      "üìä ML Training Dataset Shape: (16128, 9)\n",
      "üéØ Target Variable: Violation_Rate\n",
      "üîß Features: Facility, Hour, Day, Month, Payment Methods\n",
      "\n",
      "üìã Sample Data for Azure AutoML:\n",
      "     FAC_B  Hour Day_Name  Month  Violation_Rate  TOTAL  VIOLATION  EZPASS  \\\n",
      "0  Bayonne     0   Friday      1        7.791155   4364        332    3409   \n",
      "1  Bayonne     0   Friday      2        5.758412   3948        219    3172   \n",
      "2  Bayonne     0   Friday      3        4.959349   4920        260    3881   \n",
      "3  Bayonne     0   Friday      4        4.983574   4756        221    3798   \n",
      "4  Bayonne     0   Friday      5        5.716101   5274        352    4061   \n",
      "\n",
      "   CASH  \n",
      "0   623  \n",
      "1   557  \n",
      "2   779  \n",
      "3   737  \n",
      "4   861  \n",
      "\n",
      "‚úÖ Data exported for Azure AutoML: traffic_ml_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Now let's create your violation prediction analysis\n",
    "print(\"üö® VIOLATION RATE ANALYSIS FOR AZURE ML\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create violation rate by facility and time for ML model\n",
    "df_traffic['Hour'] = df_traffic['TIME'] // 100  # Convert to hours\n",
    "df_traffic['Violation_Rate'] = (df_traffic['VIOLATION'] / df_traffic['TOTAL']) * 100\n",
    "\n",
    "# Key features for Azure AutoML\n",
    "ml_features = df_traffic.groupby(['FAC_B', 'Hour', 'Day_Name', 'Month']).agg({\n",
    "    'Violation_Rate': 'mean',\n",
    "    'TOTAL': 'sum',\n",
    "    'VIOLATION': 'sum',\n",
    "    'EZPASS': 'sum',\n",
    "    'CASH': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"üìä ML Training Dataset Shape: {ml_features.shape}\")\n",
    "print(f\"üéØ Target Variable: Violation_Rate\")\n",
    "print(f\"üîß Features: Facility, Hour, Day, Month, Payment Methods\")\n",
    "\n",
    "# Show sample for ML\n",
    "print(\"\\nüìã Sample Data for Azure AutoML:\")\n",
    "print(ml_features.head())\n",
    "\n",
    "# Export for Azure ML\n",
    "ml_features.to_csv('../data/traffic_ml_features.csv', index=False)\n",
    "print(\"\\n‚úÖ Data exported for Azure AutoML: traffic_ml_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c5bf75-8f14-4c03-a2d0-1f5ed36093f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the full traffic dataset using the absolute path (this might take a moment)...\n",
      "Dataset loaded successfully.\n",
      "Aggregating total traffic by facility and week (W)...\n",
      "Aggregation complete.\n",
      "Columns renamed for AutoML.\n",
      "Data sorted.\n",
      "--------------------------------------------------\n",
      "Time series dataset ready for Azure AutoML!\n",
      "Shape: (5033, 3)\n",
      "Saved to: traffic_timeseries_weekly.csv\n",
      "\n",
      "First 5 rows:\n",
      "  SeriesIDColumn TimeColumn  TargetColumn\n",
      "0        Bayonne 2013-01-06         48776\n",
      "1        Bayonne 2013-01-13         64994\n",
      "2        Bayonne 2013-01-20         64052\n",
      "3        Bayonne 2013-01-27         61863\n",
      "4        Bayonne 2013-02-03         64458\n",
      "\n",
      "Last 5 rows:\n",
      "     SeriesIDColumn TimeColumn  TargetColumn\n",
      "5028    Outerbridge 2025-05-04        293735\n",
      "5029    Outerbridge 2025-05-11        295033\n",
      "5030    Outerbridge 2025-05-18        297999\n",
      "5031    Outerbridge 2025-05-25        291989\n",
      "5032    Outerbridge 2025-06-01        251947\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import pandas (if not already done)\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: LOAD THE FULL TRAFFIC DATASET ---\n",
    "# *** Using the FULL ABSOLUTE PATH you provided ***\n",
    "# The r'...' syntax handles Windows backslashes correctly\n",
    "print(\"Loading the full traffic dataset using the absolute path (this might take a moment)...\")\n",
    "file_path = r'C:\\Users\\kanha\\OneDrive\\Desktop\\PANYNJ_Project\\data\\All Recorded Traffic.txt'\n",
    "try:\n",
    "    df_traffic = pd.read_csv(file_path, sep='\\t', parse_dates=['DATE'])\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "\n",
    "    # Ensure DATE column is datetime type\n",
    "    df_traffic['DATE'] = pd.to_datetime(df_traffic['DATE'])\n",
    "\n",
    "    # --- Step 2: AGGREGATE DATA WEEKLY BY FACILITY ---\n",
    "    time_frequency = 'W' # Weekly aggregation ('W-MON' starts weeks on Monday)\n",
    "    print(f\"Aggregating total traffic by facility and week ({time_frequency})...\")\n",
    "\n",
    "    # Group by Facility (FAC_B) and Time (weekly), then sum TOTAL traffic\n",
    "    df_timeseries = df_traffic.groupby(\n",
    "        ['FAC_B', pd.Grouper(key='DATE', freq=time_frequency)]\n",
    "    )['TOTAL'].sum().reset_index()\n",
    "    print(\"Aggregation complete.\")\n",
    "\n",
    "    # --- Step 3: RENAME COLUMNS FOR AZURE AUTOML ---\n",
    "    df_timeseries.rename(columns={\n",
    "        'DATE': 'TimeColumn',          # The time stamp column\n",
    "        'TOTAL': 'TargetColumn',       # The value we want to forecast\n",
    "        'FAC_B': 'SeriesIDColumn'      # Identifies each unique time series (each facility)\n",
    "    }, inplace=True)\n",
    "    print(\"Columns renamed for AutoML.\")\n",
    "\n",
    "    # --- Step 4: SORT DATA (CRITICAL FOR TIME SERIES) ---\n",
    "    df_timeseries.sort_values(by=['SeriesIDColumn', 'TimeColumn'], inplace=True)\n",
    "    print(\"Data sorted.\")\n",
    "\n",
    "    # --- Step 5: SAVE THE TIME SERIES DATASET ---\n",
    "    # Save in the main project folder (where your notebook likely is)\n",
    "    output_filename = 'traffic_timeseries_weekly.csv'\n",
    "    df_timeseries.to_csv(output_filename, index=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Time series dataset ready for Azure AutoML!\")\n",
    "    print(f\"Shape: {df_timeseries.shape}\")\n",
    "    print(f\"Saved to: {output_filename}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_timeseries.head())\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_timeseries.tail())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(f\"Could not find the file at the specified path: {file_path}\")\n",
    "    print(\"Please double-check the path and file name.\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERROR ---\")\n",
    "    print(f\"A required column is missing: {e}\")\n",
    "    print(\"Please check the column names in 'All Recorded Traffic.txt'. Expected: 'DATE', 'FAC_B', 'TOTAL'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An unexpected error occurred ---\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26ddec1-4a12-41ed-8781-94061eb9f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the full traffic dataset from '../data/' folder (this might take a moment)...\n",
      "Dataset loaded successfully.\n",
      "Aggregating total traffic by facility and week (W)...\n",
      "Aggregation complete.\n",
      "Columns renamed for AutoML.\n",
      "Data sorted.\n",
      "--------------------------------------------------\n",
      "Time series dataset ready for Azure AutoML!\n",
      "Shape: (5033, 3)\n",
      "Saved to: ../traffic_timeseries_weekly.csv (in your main project folder)\n",
      "\n",
      "First 5 rows:\n",
      "  SeriesIDColumn TimeColumn  TargetColumn\n",
      "0        Bayonne 2013-01-06         48776\n",
      "1        Bayonne 2013-01-13         64994\n",
      "2        Bayonne 2013-01-20         64052\n",
      "3        Bayonne 2013-01-27         61863\n",
      "4        Bayonne 2013-02-03         64458\n",
      "\n",
      "Last 5 rows:\n",
      "     SeriesIDColumn TimeColumn  TargetColumn\n",
      "5028    Outerbridge 2025-05-04        293735\n",
      "5029    Outerbridge 2025-05-11        295033\n",
      "5030    Outerbridge 2025-05-18        297999\n",
      "5031    Outerbridge 2025-05-25        291989\n",
      "5032    Outerbridge 2025-06-01        251947\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import pandas (if not already done)\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: LOAD THE FULL TRAFFIC DATASET ---\n",
    "# *** Using relative path '../data/' assuming notebook is in a subfolder ***\n",
    "print(\"Loading the full traffic dataset from '../data/' folder (this might take a moment)...\")\n",
    "try:\n",
    "    # Use the '../data/' path structure\n",
    "    df_traffic = pd.read_csv('../data/All Recorded Traffic.txt', sep='\\t', parse_dates=['DATE'])\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "\n",
    "    # Ensure DATE column is datetime type\n",
    "    df_traffic['DATE'] = pd.to_datetime(df_traffic['DATE'])\n",
    "\n",
    "    # --- Step 2: AGGREGATE DATA WEEKLY BY FACILITY ---\n",
    "    time_frequency = 'W' # Weekly aggregation ('W-MON' starts weeks on Monday)\n",
    "    print(f\"Aggregating total traffic by facility and week ({time_frequency})...\")\n",
    "\n",
    "    # Group by Facility (FAC_B) and Time (weekly), then sum TOTAL traffic\n",
    "    df_timeseries = df_traffic.groupby(\n",
    "        ['FAC_B', pd.Grouper(key='DATE', freq=time_frequency)]\n",
    "    )['TOTAL'].sum().reset_index()\n",
    "    print(\"Aggregation complete.\")\n",
    "\n",
    "    # --- Step 3: RENAME COLUMNS FOR AZURE AUTOML ---\n",
    "    df_timeseries.rename(columns={\n",
    "        'DATE': 'TimeColumn',          # The time stamp column\n",
    "        'TOTAL': 'TargetColumn',       # The value we want to forecast\n",
    "        'FAC_B': 'SeriesIDColumn'      # Identifies each unique time series (each facility)\n",
    "    }, inplace=True)\n",
    "    print(\"Columns renamed for AutoML.\")\n",
    "\n",
    "    # --- Step 4: SORT DATA (CRITICAL FOR TIME SERIES) ---\n",
    "    df_timeseries.sort_values(by=['SeriesIDColumn', 'TimeColumn'], inplace=True)\n",
    "    print(\"Data sorted.\")\n",
    "\n",
    "    # --- Step 5: SAVE THE TIME SERIES DATASET ---\n",
    "    # Save in the main project folder (using '../' to go up one level)\n",
    "    output_filename = '../traffic_timeseries_weekly.csv'\n",
    "    df_timeseries.to_csv(output_filename, index=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Time series dataset ready for Azure AutoML!\")\n",
    "    print(f\"Shape: {df_timeseries.shape}\")\n",
    "    print(f\"Saved to: {output_filename} (in your main project folder)\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_timeseries.head())\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df_timeseries.tail())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"Could not find '../data/All Recorded Traffic.txt'.\")\n",
    "    print(\"This path assumes your notebook is in a subfolder (like 'notebooks/')\")\n",
    "    print(\"Check your notebook's location and the 'data' folder's position relative to it.\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERROR ---\")\n",
    "    print(f\"A required column is missing: {e}\")\n",
    "    print(\"Please check the column names in 'All Recorded Traffic.txt'. Expected: 'DATE', 'FAC_B', 'TOTAL'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An unexpected error occurred ---\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e38ff021-efb3-4f13-b0d7-acc58e4077d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Facility Mobility Speeds dataset from '../data/' folder...\n",
      "Dataset loaded successfully.\n",
      "Target variable 'IsHighCongestion' created based on Delta < -10.\n",
      "Class distribution (0=Low Congestion, 1=High Congestion):\n",
      "IsHighCongestion\n",
      "0    0.614583\n",
      "1    0.385417\n",
      "Name: proportion, dtype: float64\n",
      "Features selected for classification.\n",
      "--------------------------------------------------\n",
      "Classification dataset ready for Azure AutoML!\n",
      "Shape: (192, 6)\n",
      "Saved to: ../congestion_classification_features.csv (in your main project folder)\n",
      "\n",
      "Features: ['Facility', 'Direction', 'Freeflow', 'Month', 'Year']\n",
      "Target: IsHighCongestion\n",
      "\n",
      "First 5 rows:\n",
      "  Facility Direction  Freeflow  IsHighCongestion  Month  Year\n",
      "0       BB        EB      44.9                 0      3  2025\n",
      "1       BB        WB      45.0                 0      3  2025\n",
      "2       GB        EB      44.9                 0      3  2025\n",
      "3       GB        WB      44.9                 0      3  2025\n",
      "4      GWB        EB      46.1                 1      3  2025\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import pandas (if not already done)\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: LOAD THE FACILITY MOBILITY SPEEDS DATASET ---\n",
    "# *** Using relative path '../data/' assuming notebook is in a subfolder ***\n",
    "print(\"Loading the Facility Mobility Speeds dataset from '../data/' folder...\")\n",
    "try:\n",
    "    # Use the '../data/' path structure\n",
    "    df_speeds = pd.read_csv('../data/Facility Mobility Speeds.txt', sep='\\t', parse_dates=['Month_Year'])\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "\n",
    "    # Ensure Month_Year is datetime type\n",
    "    df_speeds['Month_Year'] = pd.to_datetime(df_speeds['Month_Year'])\n",
    "\n",
    "    # --- Step 2: ENGINEER THE TARGET VARIABLE ---\n",
    "    # Define High Congestion threshold (Delta < -10 mph)\n",
    "    congestion_threshold = -10\n",
    "    df_speeds['IsHighCongestion'] = (df_speeds['Delta'] < congestion_threshold).astype(int) # 1 if True (High Congestion), 0 if False\n",
    "\n",
    "    print(f\"Target variable 'IsHighCongestion' created based on Delta < {congestion_threshold}.\")\n",
    "    print(\"Class distribution (0=Low Congestion, 1=High Congestion):\")\n",
    "    print(df_speeds['IsHighCongestion'].value_counts(normalize=True)) # Show class distribution\n",
    "\n",
    "    # --- Step 3: SELECT FEATURES FOR CLASSIFICATION ---\n",
    "    # We will predict 'IsHighCongestion' based on other factors\n",
    "    # Drop columns not needed for prediction or that directly give away the answer (like Delta, Avg_Speed)\n",
    "    # Also drop Facility_Order as it's just for sorting\n",
    "    features_to_keep = ['Facility', 'Month_Year', 'Direction', 'Freeflow', 'IsHighCongestion']\n",
    "    df_classification = df_speeds[features_to_keep].copy()\n",
    "\n",
    "    # Optional: Extract Month and Year from Month_Year as separate features\n",
    "    df_classification['Month'] = df_classification['Month_Year'].dt.month\n",
    "    df_classification['Year'] = df_classification['Month_Year'].dt.year\n",
    "    df_classification.drop('Month_Year', axis=1, inplace=True) # Drop original date column if month/year are used\n",
    "\n",
    "    print(\"Features selected for classification.\")\n",
    "\n",
    "    # --- Step 4: SAVE THE CLASSIFICATION DATASET ---\n",
    "    # Save in the main project folder (using '../' to go up one level)\n",
    "    output_filename_class = '../congestion_classification_features.csv'\n",
    "    df_classification.to_csv(output_filename_class, index=False)\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Classification dataset ready for Azure AutoML!\")\n",
    "    print(f\"Shape: {df_classification.shape}\")\n",
    "    print(f\"Saved to: {output_filename_class} (in your main project folder)\")\n",
    "    print(\"\\nFeatures:\", [col for col in df_classification.columns if col != 'IsHighCongestion'])\n",
    "    print(\"Target:\", 'IsHighCongestion')\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_classification.head())\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"Could not find '../data/Facility Mobility Speeds.txt'.\")\n",
    "    print(\"This path assumes your notebook is in a subfolder (like 'notebooks/')\")\n",
    "    print(\"and the 'data' folder is in the parent directory ('PANYNJ_Project/').\")\n",
    "    print(\"Please check your notebook's location.\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n--- ERROR ---\")\n",
    "    print(f\"A required column is missing: {e}\")\n",
    "    print(\"Check the column names in 'Facility Mobility Speeds.txt'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An unexpected error occurred ---\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f173118c-a1d2-4856-9f6d-019096609cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kanha\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836f3606-83fc-4e96-8f51-5273c59b976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather data for NYC (40.75, -74.0) from 2013-01-01 to 2025-05-31...\n",
      "--------------------------------------------------\n",
      "Weather data fetched and processed successfully!\n",
      "Shape: (4534, 5)\n",
      "Saved to: ../nyc_weather_2013_2025.csv (in your main project folder)\n",
      "\n",
      "First 5 rows:\n",
      "        DATE  TempMax_C  TempMin_C  Precipitation_mm  Snowfall_cm\n",
      "0 2013-01-01        3.0       -3.9               0.0          0.0\n",
      "1 2013-01-02       -1.0       -7.6               0.0          0.0\n",
      "2 2013-01-03       -0.8       -9.3               0.0          0.0\n",
      "3 2013-01-04        2.1       -4.5               0.0          0.0\n",
      "4 2013-01-05        4.0       -1.8               0.0          0.0\n",
      "\n",
      "Last 5 rows:\n",
      "           DATE  TempMax_C  TempMin_C  Precipitation_mm  Snowfall_cm\n",
      "4529 2025-05-27       25.5       11.7               0.0          0.0\n",
      "4530 2025-05-28       18.8       12.3              10.6          0.0\n",
      "4531 2025-05-29       21.5       12.6               2.9          0.0\n",
      "4532 2025-05-30       23.3       16.1               7.7          0.0\n",
      "4533 2025-05-31       19.3       12.2              39.4          0.0\n",
      "\n",
      "Data Summary:\n",
      "                      DATE    TempMax_C    TempMin_C  Precipitation_mm  \\\n",
      "count                 4534  4534.000000  4534.000000       4534.000000   \n",
      "mean   2019-03-17 12:00:00    16.500684     7.864623          3.427172   \n",
      "min    2013-01-01 00:00:00   -12.800000   -21.200000          0.000000   \n",
      "25%    2016-02-08 06:00:00     8.300000     0.200000          0.000000   \n",
      "50%    2019-03-17 12:00:00    16.900000     7.800000          0.100000   \n",
      "75%    2022-04-23 18:00:00    25.300000    16.400000          2.800000   \n",
      "max    2025-05-31 00:00:00    37.400000    26.200000        187.200000   \n",
      "std                    NaN     9.984113     9.539095          8.135028   \n",
      "\n",
      "       Snowfall_cm  \n",
      "count  4534.000000  \n",
      "mean      0.183383  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max      30.800000  \n",
      "std       1.383876  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Date range based on your traffic data\n",
    "start_date_str = '2013-01-01'\n",
    "end_date_str = '2025-05-31'\n",
    "\n",
    "# Coordinates for NYC area (approximating Port Authority facilities)\n",
    "latitude = 40.75 # Near Lincoln/Holland Tunnels & PABT\n",
    "longitude = -74.00\n",
    "\n",
    "# Weather variables needed (Tmax, Tmin, Precipitation, Snow)\n",
    "# API variable names: temperature_2m_max, temperature_2m_min, precipitation_sum, snowfall_sum\n",
    "weather_variables = 'temperature_2m_max,temperature_2m_min,precipitation_sum,snowfall_sum'\n",
    "\n",
    "# --- API Call ---\n",
    "print(f\"Fetching weather data for NYC ({latitude}, {longitude}) from {start_date_str} to {end_date_str}...\")\n",
    "\n",
    "# Construct the API URL\n",
    "base_url = \"https://archive-api.open-meteo.com/v1/era5\"\n",
    "params = {\n",
    "    'latitude': latitude,\n",
    "    'longitude': longitude,\n",
    "    'start_date': start_date_str,\n",
    "    'end_date': end_date_str,\n",
    "    'daily': weather_variables,\n",
    "    'timezone': 'America/New_York'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Make the API request\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "\n",
    "    # Process the JSON response\n",
    "    weather_data_json = response.json()\n",
    "\n",
    "    if 'daily' in weather_data_json:\n",
    "        # Convert the 'daily' data into a pandas DataFrame\n",
    "        df_weather = pd.DataFrame(weather_data_json['daily'])\n",
    "\n",
    "        # Rename columns to be more descriptive\n",
    "        df_weather.rename(columns={\n",
    "            'time': 'DATE', # Match the traffic data date column name\n",
    "            'temperature_2m_max': 'TempMax_C',\n",
    "            'temperature_2m_min': 'TempMin_C',\n",
    "            'precipitation_sum': 'Precipitation_mm',\n",
    "            'snowfall_sum': 'Snowfall_cm'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Convert DATE column to datetime objects (without time)\n",
    "        df_weather['DATE'] = pd.to_datetime(df_weather['DATE']).dt.date\n",
    "        df_weather['DATE'] = pd.to_datetime(df_weather['DATE']) # Convert back to datetime64[ns] for merging\n",
    "\n",
    "        # --- Save the weather dataset ---\n",
    "        output_filename_weather = '../nyc_weather_2013_2025.csv'\n",
    "        # Save in the main project folder\n",
    "        df_weather.to_csv(output_filename_weather, index=False)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Weather data fetched and processed successfully!\")\n",
    "        print(f\"Shape: {df_weather.shape}\")\n",
    "        print(f\"Saved to: {output_filename_weather} (in your main project folder)\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df_weather.head())\n",
    "        print(\"\\nLast 5 rows:\")\n",
    "        print(df_weather.tail())\n",
    "        print(\"\\nData Summary:\")\n",
    "        print(df_weather.describe())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"Could not find 'daily' data in the API response.\")\n",
    "        print(\"Response:\", weather_data_json)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(f\"API request failed: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An unexpected error occurred ---\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "653c11e1-32af-4e3c-bf6e-5addb8012316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data merging and feature engineering script...\n",
      "Loading raw traffic data from ../data/All Recorded Traffic.txt...\n",
      "Loading weather data from ../data/nyc_weather_2013_2025.csv...\n",
      "Data loaded successfully.\n",
      "Preparing traffic data...\n",
      "Normalizing 'TIME' column (e.g., 100 -> 1)...\n",
      "Preparing weather data...\n",
      "Merging traffic and weather datasets...\n",
      "Engineering features...\n",
      "Aggregating data by facility, hour, and weather...\n",
      "Aggregation complete.\n",
      "Saving new ML-ready file to ../data/violation_ml_features_v2.csv...\n",
      "\n",
      "--- SCRIPT COMPLETE ---\n",
      "Your new file is ready at: ../data/violation_ml_features_v2.csv\n",
      "\n",
      "First 5 rows of the new data:\n",
      "     FAC_B  merge_date  Hour Day_Name  Month  TempMax_C  TempMin_C  \\\n",
      "0  Bayonne  2013-01-01     0  Tuesday      1        3.0       -3.9   \n",
      "1  Bayonne  2013-01-01     1  Tuesday      1        3.0       -3.9   \n",
      "2  Bayonne  2013-01-01     2  Tuesday      1        3.0       -3.9   \n",
      "3  Bayonne  2013-01-01     3  Tuesday      1        3.0       -3.9   \n",
      "4  Bayonne  2013-01-01     4  Tuesday      1        3.0       -3.9   \n",
      "\n",
      "   Precipitation_mm  Snowfall_cm  Violation_Count  \n",
      "0               0.0          0.0                5  \n",
      "1               0.0          0.0                1  \n",
      "2               0.0          0.0                8  \n",
      "3               0.0          0.0                2  \n",
      "4               0.0          0.0                7  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Starting data merging and feature engineering script...\")\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "traffic_file = '../data/All Recorded Traffic.txt'\n",
    "weather_file = '../data/nyc_weather_2013_2025.csv'\n",
    "output_file = '../data/violation_ml_features_v2.csv'\n",
    "\n",
    "# --- 2. Load Datasets ---\n",
    "print(f\"Loading raw traffic data from {traffic_file}...\")\n",
    "try:\n",
    "    df_traffic = pd.read_csv(traffic_file, sep='\\t')\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: Could not find traffic file at {traffic_file}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Loading weather data from {weather_file}...\")\n",
    "try:\n",
    "    df_weather = pd.read_csv(weather_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: Could not find weather file at {weather_file}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# --- 3. Prepare Data for Merging ---\n",
    "\n",
    "print(\"Preparing traffic data...\")\n",
    "df_traffic.columns = df_traffic.columns.str.strip()\n",
    "\n",
    "# --- FIX #2: Normalize the 'TIME' column ---\n",
    "# Convert 100, 200, 2300 -> 1, 2, 23\n",
    "print(\"Normalizing 'TIME' column (e.g., 100 -> 1)...\")\n",
    "df_traffic['TIME'] = (df_traffic['TIME'] / 100).astype(int)\n",
    "# --- END FIX #2 ---\n",
    "\n",
    "# Convert DATE column to datetime objects\n",
    "df_traffic['DATE'] = pd.to_datetime(df_traffic['DATE'], errors='coerce')\n",
    "df_traffic = df_traffic.dropna(subset=['DATE'])\n",
    "\n",
    "# Create the 'merge_date' column (date only, for weather merge)\n",
    "df_traffic['merge_date'] = df_traffic['DATE'].dt.date\n",
    "\n",
    "# Create the full 'timestamp' by adding the 'TIME' (hour)\n",
    "df_traffic['timestamp'] = df_traffic['DATE'] + pd.to_timedelta(df_traffic['TIME'], unit='h')\n",
    "\n",
    "\n",
    "# Prepare Weather Data\n",
    "print(\"Preparing weather data...\")\n",
    "df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "df_weather['merge_date'] = df_weather['DATE'].dt.date\n",
    "df_weather = df_weather.drop(columns=['DATE'])\n",
    "\n",
    "# --- 4. Merge Traffic and Weather Data ---\n",
    "print(\"Merging traffic and weather datasets...\")\n",
    "df_merged = pd.merge(df_traffic, df_weather, on='merge_date', how='left')\n",
    "df_merged = df_merged.dropna(subset=['TempMax_C']) \n",
    "\n",
    "# --- 5. Feature Engineering ---\n",
    "print(\"Engineering features...\")\n",
    "df_merged['Hour'] = df_merged['TIME'] # Rename for clarity\n",
    "\n",
    "if 'VIOLATION' not in df_merged.columns:\n",
    "    print(\"FATAL ERROR: 'VIOLATION' column not found after merge.\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. Aggregate Data to Create ML-Ready File ---\n",
    "print(\"Aggregating data by facility, hour, and weather...\")\n",
    "\n",
    "grouping_cols = [\n",
    "    'FAC_B',\n",
    "    'merge_date', \n",
    "    'Hour',\n",
    "    'Day_Name',\n",
    "    'Month',\n",
    "    'TempMax_C',\n",
    "    'TempMin_C',\n",
    "    'Precipitation_mm',\n",
    "    'Snowfall_cm'\n",
    "]\n",
    "\n",
    "aggregation = {\n",
    "    'VIOLATION': 'sum'\n",
    "}\n",
    "\n",
    "df_final = df_merged.groupby(grouping_cols).agg(aggregation).reset_index()\n",
    "\n",
    "# Rename 'VIOLATION' to 'Violation_Count'\n",
    "df_final = df_final.rename(columns={'VIOLATION': 'Violation_Count'})\n",
    "\n",
    "print(\"Aggregation complete.\")\n",
    "\n",
    "# --- 7. Save the New Dataset ---\n",
    "print(f\"Saving new ML-ready file to {output_file}...\")\n",
    "output_dir = os.path.dirname(output_file)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n--- SCRIPT COMPLETE ---\")\n",
    "print(f\"Your new file is ready at: {output_file}\")\n",
    "print(\"\\nFirst 5 rows of the new data:\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a58762f-2870-4af3-a0c4-2261bf2dbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script to create new ML-ready time series CSV...\n",
      "Loading weekly traffic data from ../data/traffic_timeseries_weekly.csv...\n",
      "Traffic data loaded.\n",
      "Loading daily weather data from ../data/nyc_weather_2013_2025.csv...\n",
      "Weather data loaded.\n",
      "Aggregating daily weather to weekly (Sunday-ending)...\n",
      "Weather data aggregated.\n",
      "Merging traffic and weekly weather data...\n",
      "Data merged.\n",
      "\n",
      "--- SCRIPT COMPLETE ---\n",
      "New ML-ready file for AutoML is saved at: ../data/traffic_timeseries_for_automl.csv\n",
      "\n",
      "First 5 rows of the new data:\n",
      "  Facility       Date  Traffic_Count  Temp_Max_Mean  Temp_Min_Mean  \\\n",
      "0  Bayonne 2013-01-06          48776       2.100000      -4.966667   \n",
      "1  Bayonne 2013-01-13          64994       7.671429      -0.242857   \n",
      "2  Bayonne 2013-01-20          64052       6.342857      -1.157143   \n",
      "3  Bayonne 2013-01-27          61863      -2.242857      -8.700000   \n",
      "4  Bayonne 2013-02-03          64458       5.228571      -2.414286   \n",
      "\n",
      "   Precip_Sum_mm  Snow_Sum_cm  \n",
      "0            0.4         0.35  \n",
      "1           11.3         0.00  \n",
      "2           20.1         6.30  \n",
      "3            1.2         1.12  \n",
      "4           21.6         2.10  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanha\\AppData\\Local\\Temp\\ipykernel_30468\\3101142914.py:56: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_merged = df_merged.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Starting script to create new ML-ready time series CSV...\")\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "traffic_path = '../data/traffic_timeseries_weekly.csv'\n",
    "weather_path = '../data/nyc_weather_2013_2025.csv'\n",
    "output_path = '../data/traffic_timeseries_for_automl.csv'\n",
    "\n",
    "try:\n",
    "    # --- 2. Load Traffic Data ---\n",
    "    print(f\"Loading weekly traffic data from {traffic_path}...\")\n",
    "    df_traffic = pd.read_csv(traffic_path)\n",
    "    \n",
    "    # Rename original columns for clarity\n",
    "    df_traffic = df_traffic.rename(columns={\n",
    "        'SeriesIDColumn': 'Facility',\n",
    "        'TimeColumn': 'Date',\n",
    "        'TargetColumn': 'Traffic_Count'\n",
    "    })\n",
    "    \n",
    "    # Convert Date to datetime for merging\n",
    "    df_traffic['Date'] = pd.to_datetime(df_traffic['Date'])\n",
    "    print(\"Traffic data loaded.\")\n",
    "\n",
    "    # --- 3. Load Weather Data ---\n",
    "    print(f\"Loading daily weather data from {weather_path}...\")\n",
    "    df_weather = pd.read_csv(weather_path)\n",
    "    \n",
    "    # Convert DATE to datetime for resampling\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    print(\"Weather data loaded.\")\n",
    "\n",
    "    # --- 4. Aggregate Weather to Weekly ---\n",
    "    # Resample to 'W-SUN' (weekly, Sunday-ending) to match the traffic data\n",
    "    print(\"Aggregating daily weather to weekly (Sunday-ending)...\")\n",
    "    df_weather_weekly = df_weather.resample('W-SUN', on='DATE').agg(\n",
    "        Temp_Max_Mean=('TempMax_C', 'mean'),\n",
    "        Temp_Min_Mean=('TempMin_C', 'mean'),\n",
    "        Precip_Sum_mm=('Precipitation_mm', 'sum'),\n",
    "        Snow_Sum_cm=('Snowfall_cm', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename the new weekly 'DATE' column to 'Date' for merging\n",
    "    df_weather_weekly = df_weather_weekly.rename(columns={'DATE': 'Date'})\n",
    "    print(\"Weather data aggregated.\")\n",
    "\n",
    "    # --- 5. Merge Traffic and Weather ---\n",
    "    print(\"Merging traffic and weekly weather data...\")\n",
    "    # Use a left merge to ensure all traffic records are kept\n",
    "    df_merged = pd.merge(df_traffic, df_weather_weekly, on='Date', how='left')\n",
    "    \n",
    "    # Fill any NaNs created by the merge (e.g., missing weather data)\n",
    "    df_merged = df_merged.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"Data merged.\")\n",
    "\n",
    "    # --- 6. Save the New Dataset ---\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    df_merged.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n--- SCRIPT COMPLETE ---\")\n",
    "    print(f\"New ML-ready file for AutoML is saved at: {output_path}\")\n",
    "    print(\"\\nFirst 5 rows of the new data:\")\n",
    "    print(df_merged.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFATAL ERROR: File not found.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(f\"Please ensure your files are located at {traffic_path} and {weather_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "463db601-897d-4a59-a0b4-ee8f4df3dcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script to create new ML-ready classification CSV...\n",
      "Loading base features from ../data/congestion_classification_features.csv...\n",
      "Loading speed data from ../data/Facility Mobility Speeds.txt...\n",
      "Loading daily weather data from ../data/nyc_weather_2013_2025.csv...\n",
      "All data loaded.\n",
      "Preparing speed data for merge...\n",
      "Preparing 2025 weather data...\n",
      "Weather data aggregated.\n",
      "Merging speed features into base features...\n",
      "Merging monthly weather data...\n",
      "All data merged.\n",
      "\n",
      "--- SCRIPT COMPLETE ---\n",
      "New ML-ready file for classification is saved at: ../data/congestion_classification_for_automl.csv\n",
      "\n",
      "First 5 rows of the new data:\n",
      "  Facility Direction  Freeflow  IsHighCongestion  Month  Year  Avg_Speed  \\\n",
      "0       BB        EB      44.9                 0      3  2025       46.9   \n",
      "1       BB        WB      45.0                 0      3  2025       48.4   \n",
      "2       GB        EB      44.9                 0      3  2025       44.6   \n",
      "3       GB        WB      44.9                 0      3  2025       47.9   \n",
      "4      GWB        EB      46.1                 1      3  2025       30.5   \n",
      "\n",
      "   Delta  Temp_Max_Mean_2025  Temp_Min_Mean_2025  Precip_Sum_mm_2025  \\\n",
      "0    2.0           12.406452            1.993548               162.5   \n",
      "1    3.4           12.406452            1.993548               162.5   \n",
      "2   -0.3           12.406452            1.993548               162.5   \n",
      "3    3.0           12.406452            1.993548               162.5   \n",
      "4  -23.2           12.406452            1.993548               162.5   \n",
      "\n",
      "   Snow_Sum_cm_2025  \n",
      "0               0.0  \n",
      "1               0.0  \n",
      "2               0.0  \n",
      "3               0.0  \n",
      "4               0.0  \n",
      "\n",
      "Columns in new file:\n",
      "['Facility', 'Direction', 'Freeflow', 'IsHighCongestion', 'Month', 'Year', 'Avg_Speed', 'Delta', 'Temp_Max_Mean_2025', 'Temp_Min_Mean_2025', 'Precip_Sum_mm_2025', 'Snow_Sum_cm_2025']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanha\\AppData\\Local\\Temp\\ipykernel_30468\\1435753831.py:73: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_final = df_final.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Starting script to create new ML-ready classification CSV...\")\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "base_features_path = '../data/congestion_classification_features.csv'\n",
    "speeds_path = '../data/Facility Mobility Speeds.txt'\n",
    "weather_path = '../data/nyc_weather_2013_2025.csv'\n",
    "output_path = '../data/congestion_classification_for_automl.csv'\n",
    "\n",
    "try:\n",
    "    # --- 2. Load Base Datasets ---\n",
    "    print(f\"Loading base features from {base_features_path}...\")\n",
    "    df_cong = pd.read_csv(base_features_path)\n",
    "    \n",
    "    print(f\"Loading speed data from {speeds_path}...\")\n",
    "    # Assuming tab-separated, adjust if necessary\n",
    "    df_speeds = pd.read_csv(speeds_path, sep='\\t')\n",
    "    \n",
    "    print(f\"Loading daily weather data from {weather_path}...\")\n",
    "    df_weather = pd.read_csv(weather_path)\n",
    "    print(\"All data loaded.\")\n",
    "\n",
    "    # --- 3. Prepare Speed Data ---\n",
    "    # Parse 'Month_Year' to get 'Month' and 'Year' for merging\n",
    "    print(\"Preparing speed data for merge...\")\n",
    "    df_speeds['Month_Year_dt'] = pd.to_datetime(df_speeds['Month_Year'])\n",
    "    df_speeds['Month'] = df_speeds['Month_Year_dt'].dt.month\n",
    "    df_speeds['Year'] = df_speeds['Month_Year_dt'].dt.year\n",
    "    # Select only the new features we want to add\n",
    "    cols_to_merge = ['Facility', 'Direction', 'Month', 'Year', 'Avg_Speed', 'Delta']\n",
    "    \n",
    "    # --- 4. Prepare Weather Data ---\n",
    "    # Filter weather for 2025 ONLY and aggregate by Month\n",
    "    print(\"Preparing 2025 weather data...\")\n",
    "    df_weather['DATE'] = pd.to_datetime(df_weather['DATE'])\n",
    "    df_weather_2025 = df_weather[df_weather['DATE'].dt.year == 2025].copy()\n",
    "    \n",
    "    # Add 'Month' column for grouping\n",
    "    df_weather_2025['Month'] = df_weather_2025['DATE'].dt.month\n",
    "    \n",
    "    # Aggregate daily 2025 weather to monthly\n",
    "    df_weather_monthly = df_weather_2025.groupby('Month').agg(\n",
    "        Temp_Max_Mean_2025=('TempMax_C', 'mean'),\n",
    "        Temp_Min_Mean_2025=('TempMin_C', 'mean'),\n",
    "        Precip_Sum_mm_2025=('Precipitation_mm', 'sum'),\n",
    "        Snow_Sum_cm_2025=('Snowfall_cm', 'sum')\n",
    "    ).reset_index()\n",
    "    print(\"Weather data aggregated.\")\n",
    "\n",
    "    # --- 5. Merge Datasets ---\n",
    "    print(\"Merging speed features into base features...\")\n",
    "    # Merge the base features with the new speed features\n",
    "    df_merged = pd.merge(\n",
    "        df_cong, \n",
    "        df_speeds[cols_to_merge], \n",
    "        on=['Facility', 'Direction', 'Month', 'Year'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"Merging monthly weather data...\")\n",
    "    # Merge the result with the monthly 2025 weather\n",
    "    df_final = pd.merge(\n",
    "        df_merged, \n",
    "        df_weather_monthly, \n",
    "        on='Month', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Handle any potential NaNs from merges\n",
    "    df_final = df_final.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"All data merged.\")\n",
    "\n",
    "    # --- 6. Save the New Dataset ---\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n--- SCRIPT COMPLETE ---\")\n",
    "    print(f\"New ML-ready file for classification is saved at: {output_path}\")\n",
    "    print(\"\\nFirst 5 rows of the new data:\")\n",
    "    print(df_final.head())\n",
    "    print(\"\\nColumns in new file:\")\n",
    "    print(list(df_final.columns))\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFATAL ERROR: File not found.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(\"Please ensure your files are located in the '../data/' directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4745f86c-5ad5-4938-8da3-25b363d01df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kanha\\anaconda3\\lib\\site-packages (2.2.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kanha\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kanha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 8.1/72.0 MB 46.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 20.4/72.0 MB 52.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 35.4/72.0 MB 58.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 50.6/72.0 MB 61.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 66.6/72.0 MB 64.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 65.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 50.5 MB/s eta 0:00:00\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 39.7 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm\n",
      "\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   ---------------------------------------- 0/2 [xgboost]\n",
      "   -------------------- ------------------- 1/2 [lightgbm]\n",
      "   -------------------- ------------------- 1/2 [lightgbm]\n",
      "   ---------------------------------------- 2/2 [lightgbm]\n",
      "\n",
      "Successfully installed lightgbm-4.6.0 xgboost-3.1.1\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn xgboost lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf19d1f-bf43-45e7-a900-a3aa4205daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Feature engineering complete.\n",
      "Building model pipeline...\n",
      "Training the final model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017971 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1304\n",
      "[LightGBM] [Info] Number of data points in the train set: 663027, number of used features: 48\n",
      "[LightGBM] [Info] Start training from score 97.933850\n",
      "Model training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanha\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model 1 (Violation) Re-creation Complete ---\n",
      "R-squared (R2) score on test set: 0.7565\n",
      "(The original AutoML R2 was: 0.981)\n",
      "Final model saved as 'violation_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def load_and_prep_data(path):\n",
    "    \"\"\"\n",
    "    Loads data and creates date features to replicate\n",
    "    Azure's TimeSeriesTransformer.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    # Re-create Azure's TimeSeriesTransformer logic\n",
    "    data['merge_date'] = pd.to_datetime(data['merge_date'])\n",
    "    data['year'] = data['merge_date'].dt.year\n",
    "    data['month'] = data['merge_date'].dt.month\n",
    "    data['day'] = data['merge_date'].dt.day\n",
    "    data['day_of_year'] = data['merge_date'].dt.dayofyear\n",
    "    data['week_of_year'] = data['merge_date'].dt.isocalendar().week\n",
    "    \n",
    "    print(\"Feature engineering complete.\")\n",
    "    return data\n",
    "\n",
    "def build_model_pipeline():\n",
    "    \"\"\"\n",
    "    Builds the full scikit-learn pipeline, including preprocessing\n",
    "    and the final VotingRegressor with exact hyperparameters.\n",
    "    \"\"\"\n",
    "    print(\"Building model pipeline...\")\n",
    "    \n",
    "    # --- 1. Define Preprocessing Steps ---\n",
    "    \n",
    "    # We group features just like the Azure script\n",
    "    \n",
    "    # Numerical features: weather + new date features\n",
    "    numeric_features = [\n",
    "        'TempMin_C', 'TempMax_C', 'Precipitation_mm', 'Snowfall_cm',\n",
    "        'year', 'month', 'day', 'day_of_year', 'week_of_year'\n",
    "    ]\n",
    "    \n",
    "    # Categorical features\n",
    "    categorical_features = ['FAC_B', 'Hour', 'Day_Name']\n",
    "\n",
    "    # Create transformers\n",
    "    # (Using Median Imputer as specified in the Azure blueprint)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # (Replaces CountVectorizer with OneHotEncoder for a standard, robust solution)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Create the master preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop' # Drop any columns we didn't specify\n",
    "    )\n",
    "\n",
    "    # --- 2. Define Base Models (Copied from your script.py) ---\n",
    "    \n",
    "    # These hyperparameters are copied directly from your\n",
    "    # auto-generated 'script.py' file.\n",
    "    \n",
    "    xgb_1 = XGBRegressor(\n",
    "        base_score=0.5, booster='gbtree', colsample_bytree=0.7,\n",
    "        learning_rate=0.1, max_depth=6, min_child_weight=1,\n",
    "        n_estimators=100, n_jobs=0, objective='reg:squarederror',\n",
    "        random_state=0, reg_alpha=0, reg_lambda=0.9, \n",
    "        subsample=0.8, tree_method='auto'\n",
    "    )\n",
    "    \n",
    "    xgb_2 = XGBRegressor(\n",
    "        base_score=0.5, booster='gbtree', colsample_bytree=0.6,\n",
    "        learning_rate=0.01, max_depth=7, min_child_weight=1,\n",
    "        n_estimators=25, n_jobs=0, objective='reg:squarederror',\n",
    "        random_state=0, reg_alpha=0.6, reg_lambda=0.9, \n",
    "        subsample=1.0, tree_method='auto'\n",
    "    )\n",
    "    \n",
    "    lgbm_1 = LGBMRegressor(\n",
    "        boosting_type='gbdt', n_estimators=100, n_jobs=-1,\n",
    "        random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
    "        subsample=1.0, colsample_bytree=1.0\n",
    "    )\n",
    "    \n",
    "    xgb_3 = XGBRegressor(\n",
    "        base_score=0.5, booster='gbtree', colsample_bytree=0.7,\n",
    "        learning_rate=0.1, max_depth=7, min_child_weight=1,\n",
    "        n_estimators=50, n_jobs=0, objective='reg:squarederror',\n",
    "        random_state=0, reg_alpha=0, reg_lambda=0.3,\n",
    "        subsample=0.8, tree_method='auto'\n",
    "    )\n",
    "\n",
    "    # --- 3. Create the Final VotingRegressor ---\n",
    "    \n",
    "    voting_model = VotingRegressor(\n",
    "        estimators=[\n",
    "            ('model_0', xgb_1),\n",
    "            ('model_1', xgb_2),\n",
    "            ('model_2', lgbm_1),\n",
    "            ('model_3', xgb_3)\n",
    "        ],\n",
    "        weights=None # Default is equal weighting\n",
    "    )\n",
    "\n",
    "    # --- 4. Create the Full Pipeline ---\n",
    "    \n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', voting_model)\n",
    "    ])\n",
    "    \n",
    "    return final_pipeline\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    DATA_PATH = '../data/violation_ml_features_v2.csv'\n",
    "    TARGET_COLUMN = 'Violation_Count'\n",
    "    \n",
    "    # 1. Load and prep data\n",
    "    data = load_and_prep_data(DATA_PATH)\n",
    "    \n",
    "    # 2. Define X (features) and y (target)\n",
    "    y = data[TARGET_COLUMN]\n",
    "    X = data.drop(columns=[TARGET_COLUMN, 'merge_date']) # Drop original date\n",
    "    \n",
    "    # 3. Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 4. Build the model pipeline\n",
    "    model = build_model_pipeline()\n",
    "    \n",
    "    # 5. Train the model\n",
    "    print(\"Training the final model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "    \n",
    "    # 6. Evaluate the model\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(f\"\\n--- Model 1 (Violation) Re-creation Complete ---\")\n",
    "    print(f\"R-squared (R2) score on test set: {score:.4f}\")\n",
    "    print(f\"(The original AutoML R2 was: 0.981)\")\n",
    "    \n",
    "    # 7. Save the final model (optional, but good practice)\n",
    "    with open('violation_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Final model saved as 'violation_model.pkl'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f670004-3395-4eba-9b46-216f75e17614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
